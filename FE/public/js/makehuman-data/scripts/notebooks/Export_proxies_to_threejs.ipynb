{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we load proxy in threejs when there is no proxy class. Here's a simple idea:\n",
    "\n",
    "What if we just attach it to the human object and map it to the bones that the weighted vertices are attached to? That would make offsets work, I'm not sure if it would make distortion work. Might be worth testing\n",
    "- get a proxy\n",
    "- collect weighting of attached vertices, merge dups, get the 4 most significant\n",
    "- export as json object with skin weights, attach to human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:44.055771",
     "start_time": "2016-12-12T17:26:44.053834"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:44.403621",
     "start_time": "2016-12-12T17:26:44.207303"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from path import Path\n",
    "import json\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "%precision 6\n",
    "\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "basedir=r'/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-05T17:40:00.699300",
     "start_time": "2016-12-05T17:40:00.696455"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.503337",
     "start_time": "2016-12-12T17:26:44.526115"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized logging\n"
     ]
    }
   ],
   "source": [
    "# from .mh_helpers import clean, short_hash, clean_modifier\n",
    "\n",
    "mhpath = Path(os.path.abspath(\"../vendor/makehuman-commandline/makehuman\"))\n",
    "\n",
    "#===============================================================================\n",
    "# Import Makehuman resources, needs to be with makehuman dir as current dir\n",
    "#===============================================================================\n",
    "\n",
    "appcwd = os.path.abspath(os.curdir)\n",
    "sys.path.append(mhpath)\n",
    "sys.path.append(appcwd)\n",
    "sys.path.append('.')\n",
    "\n",
    "def getHuman():\n",
    "    \"\"\"Load a human model with modifiers.\"\"\"\n",
    "    with mhpath:\n",
    "        # maxFaces *uint* Number of faces per vertex (pole), None for default (min 4)\n",
    "        human = Human(files3d.loadMesh(\n",
    "            getpath.getSysDataPath(\"3dobjs/base.obj\"),\n",
    "            maxFaces=5))\n",
    "        # load modifiers onto human\n",
    "        humanmodifier.mods_loaded = False\n",
    "        modifiers = humanmodifier.loadModifiers(\n",
    "            getpath.getSysDataPath('modifiers/modeling_modifiers.json'), human)\n",
    "        return human\n",
    "\n",
    "with mhpath:\n",
    "    import makehuman\n",
    "    oldpath = os.sys.path\n",
    "    makehuman.set_sys_path()\n",
    "    # make makehuman paths absolute by going through newest paths and making abs\n",
    "    for i in range(len(os.sys.path)):\n",
    "        p = os.sys.path[i]\n",
    "        if p[0:2] == './':\n",
    "            os.sys.path[i] = os.path.join(\n",
    "                os.path.abspath('.'), p.replace('./', ''))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    makehuman.init_logging()\n",
    "    logging.getLogger().setLevel(logging.CRITICAL)\n",
    "    #import image_pil as image_lib\n",
    "    #\n",
    "    import proxy as mhproxy\n",
    "    import humanargparser\n",
    "    import targets as mhtargets\n",
    "    from human import Human\n",
    "    import files3d\n",
    "    import getpath\n",
    "    import humanmodifier\n",
    "    from core import G\n",
    "    import headless\n",
    "    import autoskinblender\n",
    "    import export\n",
    "    \n",
    "    # Init console app\n",
    "    with mhpath:\n",
    "        G.app = headless.ConsoleApp()\n",
    "    G.app.selectedHuman = human = getHuman()\n",
    "    headless.OBJExporter = None\n",
    "    headless.MHXExporter = None\n",
    "    headless.MhxConfig = None\n",
    "    humanargparser.mods_loaded = False\n",
    "    \n",
    "    from makehuman import LicenseInfo\n",
    "    mh_licence=LicenseInfo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.565562",
     "start_time": "2016-12-12T17:26:47.504702"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import proxies\n",
    "def _listDataFiles(foldername,\n",
    "                   extensions,\n",
    "                   onlySysData=False,\n",
    "                   recursive=True):\n",
    "    with mhpath:  # sadly makehuman seems hardcoded\n",
    "        if onlySysData:\n",
    "            paths = [getpath.getSysDataPath(foldername)]\n",
    "        else:\n",
    "            paths = [getpath.getDataPath(foldername),\n",
    "                     getpath.getSysDataPath(foldername)]\n",
    "    return list(getpath.search(paths, extensions, recursive))\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"Remove invalid characters.\"\"\"\n",
    "    s = re.sub('[^0-9a-zA-Z_]', '_', s)\n",
    "    return s\n",
    "\n",
    "with mhpath:\n",
    "    mhproxy.ProxyTypes\n",
    "    proxies = OrderedDict()\n",
    "    for proxyType in mhproxy.ProxyTypes+['Genitals']:\n",
    "        files = list(_listDataFiles(proxyType.lower(),\n",
    "                                         ['.proxy', '.mhclo']))\n",
    "        for f in files:\n",
    "            if proxyType not in proxies.keys():\n",
    "                proxies[proxyType] = OrderedDict()\n",
    "            filesname = clean(os.path.splitext(os.path.basename(f))[0])\n",
    "            proxies[proxyType][filesname] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.577339",
     "start_time": "2016-12-12T17:26:47.566910"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(u'f_top_01', u'data/clothes/female_top_01/f_top_01.mhclo'),\n",
       "             (u'male_casualsuit01',\n",
       "              u'data/clothes/male_casualsuit01/male_casualsuit01.mhclo'),\n",
       "             (u'asymmetric_tunic_and_sash',\n",
       "              u'data/clothes/asymmetric_tunic_and_sash/asymmetric_tunic_and_sash.mhclo'),\n",
       "             (u'babydoll', u'data/clothes/babydoll/babydoll.mhclo'),\n",
       "             (u'bandbikinibra',\n",
       "              u'data/clothes/bandeau_bikini_(bra)/bandbikinibra.mhclo'),\n",
       "             (u'bandbikinislip',\n",
       "              u'data/clothes/bandeau_bikini_(slip)/bandbikinislip.mhclo'),\n",
       "             (u'bandeau_bra', u'data/clothes/bandeau_bra/bandeau_bra.mhclo'),\n",
       "             (u'cocktaildress',\n",
       "              u'data/clothes/black_cocktaildress/cocktaildress.mhclo'),\n",
       "             (u'blackminiskirt',\n",
       "              u'data/clothes/black_mini_skirt/blackminiskirt.mhclo'),\n",
       "             (u'rolled_neck_blouse',\n",
       "              u'data/clothes/blouse/rolled_neck_blouse.mhclo'),\n",
       "             (u'brazbikinibra',\n",
       "              u'data/clothes/brazilian_bikini_(bra)/brazbikinibra.mhclo'),\n",
       "             (u'brazbikinislip',\n",
       "              u'data/clothes/brazilian_bikini_(slip)/brazbikinislip.mhclo'),\n",
       "             (u'fedora', u'data/clothes/fedora01/fedora.mhclo'),\n",
       "             (u'fedora_cocked', u'data/clothes/fedora01/fedora_cocked.mhclo'),\n",
       "             (u'female_casualsuit01',\n",
       "              u'data/clothes/female_casualsuit01/female_casualsuit01.mhclo'),\n",
       "             (u'female_casualsuit02',\n",
       "              u'data/clothes/female_casualsuit02/female_casualsuit02.mhclo'),\n",
       "             (u'female_elegantsuit01',\n",
       "              u'data/clothes/female_elegantsuit01/female_elegantsuit01.mhclo'),\n",
       "             (u'f_panties_01',\n",
       "              u'data/clothes/female_panties_01/f_panties_01.mhclo'),\n",
       "             (u'female_sportsuit01',\n",
       "              u'data/clothes/female_sportsuit01/female_sportsuit01.mhclo'),\n",
       "             (u'toenail2', u'data/clothes/female_toenails/toenail2.mhclo'),\n",
       "             (u'male_casualsuit02',\n",
       "              u'data/clothes/male_casualsuit02/male_casualsuit02.mhclo'),\n",
       "             (u'male_casualsuit03',\n",
       "              u'data/clothes/male_casualsuit03/male_casualsuit03.mhclo'),\n",
       "             (u'male_casualsuit04',\n",
       "              u'data/clothes/male_casualsuit04/male_casualsuit04.mhclo'),\n",
       "             (u'male_casualsuit05',\n",
       "              u'data/clothes/male_casualsuit05/male_casualsuit05.mhclo'),\n",
       "             (u'male_casualsuit06',\n",
       "              u'data/clothes/male_casualsuit06/male_casualsuit06.mhclo'),\n",
       "             (u'male_elegantsuit01',\n",
       "              u'data/clothes/male_elegantsuit01/male_elegantsuit01.mhclo'),\n",
       "             (u'male_worksuit01',\n",
       "              u'data/clothes/male_worksuit01/male_worksuit01.mhclo'),\n",
       "             (u'pith_helmet', u'data/clothes/pith_helmet/pith_helmet.mhclo'),\n",
       "             (u'rucksack1',\n",
       "              u'data/clothes/rucksack_/_backpack/rucksack1.mhclo'),\n",
       "             (u'sari1', u'data/clothes/sari_one/sari1.mhclo'),\n",
       "             (u'shoes01', u'data/clothes/shoes01/shoes01.mhclo'),\n",
       "             (u'shoes02', u'data/clothes/shoes02/shoes02.mhclo'),\n",
       "             (u'shoes03', u'data/clothes/shoes03/shoes03.mhclo'),\n",
       "             (u'shoes04', u'data/clothes/shoes04/shoes04.mhclo'),\n",
       "             (u'shoes05', u'data/clothes/shoes05/shoes05.mhclo'),\n",
       "             (u'shoes06', u'data/clothes/shoes06/shoes06.mhclo'),\n",
       "             (u'short_tail_camo_tee',\n",
       "              u'data/clothes/short-tail_camo_tee/short_tail_camo_tee.mhclo'),\n",
       "             (u'shortjeans',\n",
       "              u'data/clothes/short_jeans_(female)/shortjeans.mhclo'),\n",
       "             (u'sleeveless',\n",
       "              u'data/clothes/sleeveless_shirt/sleeveless.mhclo'),\n",
       "             (u'spaghetti_top',\n",
       "              u'data/clothes/spaghetti_strap_tank_top/spaghetti_top.mhclo'),\n",
       "             (u'tank_top_01', u'data/clothes/tank_top_01/tank_top_01.mhclo'),\n",
       "             (u'teddy', u'data/clothes/teddy_(erotic_lingerie)/teddy.mhclo'),\n",
       "             (u'tightjeans',\n",
       "              u'data/clothes/tight_jeans_(female)/tightjeans.mhclo'),\n",
       "             (u'toenail',\n",
       "              u'data/clothes/toenails_female_dark_red_/_natural/toenail.mhclo'),\n",
       "             (u'transparent_bra',\n",
       "              u'data/clothes/transparent_bra/transparent_bra.mhclo'),\n",
       "             (u'tubedress', u'data/clothes/tube_dress_/tubedress.mhclo'),\n",
       "             (u'tubetop', u'data/clothes/tube_top/tubetop.mhclo'),\n",
       "             (u'vnecktop', u'data/clothes/v_neck_top/vnecktop.mhclo'),\n",
       "             (u'winterboots', u'data/clothes/winter_boots/winterboots.mhclo'),\n",
       "             (u'coat', u'data/clothes/winter_coat/coat.mhclo'),\n",
       "             (u'facehelpers',\n",
       "              u'data/clothes/__helpersdebug/facehelpers.mhclo'),\n",
       "             (u'genitalshelper',\n",
       "              u'data/clothes/__helpersdebug/genitalshelper.mhclo'),\n",
       "             (u'hairhelper', u'data/clothes/__helpersdebug/hairhelper.mhclo'),\n",
       "             (u'jointshelpers',\n",
       "              u'data/clothes/__helpersdebug/jointshelpers.mhclo'),\n",
       "             (u'skirthelper',\n",
       "              u'data/clothes/__helpersdebug/skirthelper.mhclo'),\n",
       "             (u'tightshelper',\n",
       "              u'data/clothes/__helpersdebug/tightshelper.mhclo'),\n",
       "             (u'fishing_hat_01',\n",
       "              u'data/clothes/fishing_hat_01/fishing_hat_01.mhclo'),\n",
       "             (u'frenchbra',\n",
       "              u'data/clothes/french_lingerie_(bra)/frenchbra.mhclo'),\n",
       "             (u'string3',\n",
       "              u'data/clothes/french_lingerie_(string)/string3.mhclo'),\n",
       "             (u'f_beach_dress_01',\n",
       "              u'data/clothes/f_beach_dress_01/f_beach_dress_01.mhclo'),\n",
       "             (u'f_bikini_01', u'data/clothes/f_bikini_01/f_bikini_01.mhclo'),\n",
       "             (u'f_dress_01', u'data/clothes/f_dress_01/f_dress_01.mhclo'),\n",
       "             (u'f_dress_02', u'data/clothes/f_dress_02/f_dress_02.mhclo'),\n",
       "             (u'f_dress_03', u'data/clothes/f_dress_03/f_dress_03.mhclo'),\n",
       "             (u'f_dress_04', u'data/clothes/f_dress_04/f_dress_04.mhclo'),\n",
       "             (u'f_one_piece_swimsuit_01',\n",
       "              u'data/clothes/f_one-piece_swimsuit_01/f_one_piece_swimsuit_01.mhclo'),\n",
       "             (u'glasses', u'data/clothes/glasses_(modern_form)/glasses.mhclo'),\n",
       "             (u'harveycapev1',\n",
       "              u'data/clothes/harveyspacecapev1/harveycapev1.mhclo'),\n",
       "             (u'harvey_sweater2',\n",
       "              u'data/clothes/harvey_croptopsweater1/harvey_sweater2.mhclo'),\n",
       "             (u'harvey_madscientistglovesv1',\n",
       "              u'data/clothes/harvey_madscientistglovesv1/harvey_madscientistglovesv1.mhclo'),\n",
       "             (u'harvey_pantsbootsv1',\n",
       "              u'data/clothes/harvey_pantsbootsv1/harvey_pantsbootsv1.mhclo'),\n",
       "             (u'harvey_vestv1',\n",
       "              u'data/clothes/harvey_vestv1/harvey_vestv1.mhclo'),\n",
       "             (u'jeansskirt', u'data/clothes/jeans_skirt/jeansskirt.mhclo'),\n",
       "             (u'armsleeve_fishnet_medium',\n",
       "              u'data/clothes/lingerie_armsleeves_black_fishnet_medium/armsleeve_fishnet_medium.mhclo'),\n",
       "             (u'armsleeve_white_s',\n",
       "              u'data/clothes/lingerie_armsleeves_white_fishnet_small/armsleeve_white_s.mhclo')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxies['Clothes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.624164",
     "start_time": "2016-12-12T17:26:47.580394"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.sys.path.append(basedir)\n",
    "from convert_obj_three import convert_ascii, parse_mtl\n",
    "from export_makehuman import material_to_mtl, vertex_weights_to_skin_weights, parse_skeleton_bones, NP_MH_Encoder, copyAndCompress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.652713",
     "start_time": "2016-12-12T17:26:47.626283"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also export\n",
    "\n",
    "# TODO compress this, it's too big. Could convert to Int16, or limit decimal places, or restrict information\n",
    "def get_proxy_metadata(prxy):\n",
    "    \"\"\"A function to get the metadata we wish to add to the json file\"\"\"\n",
    "    data=dict(\n",
    "        description=prxy.description\n",
    "    )\n",
    "    keys=[\n",
    "#      '_material_file',\n",
    "#      '_obj_file',\n",
    "#      '_vertexBoneWeights_file',\n",
    "     'basemesh',\n",
    "     'deleteVerts',\n",
    "     'description',\n",
    "     'file',\n",
    "    # #  'human',\n",
    "     'license',\n",
    "    # #  'material',\n",
    "#      'max_pole',\n",
    "#      'mtime',\n",
    "#      'name',\n",
    "    # #  'object',\n",
    "     'offsets',\n",
    "     'ref_vIdxs',\n",
    "     'tags',\n",
    "#      'tmatrix',\n",
    "     'type',\n",
    "     'uuid',\n",
    "#      'uvLayers',\n",
    "     'version',\n",
    "#      'vertWeights',\n",
    "#      'vertexBoneWeights',\n",
    "     'weights',\n",
    "     'z_depth'\n",
    "    ]\n",
    "    for key in keys:\n",
    "        v = getattr(prxy,key)\n",
    "\n",
    "        # keys must be strings\n",
    "        if isinstance(v,dict):\n",
    "            if v.keys() and not isinstance(v.keys()[0],str):\n",
    "                v = dict((str(k),vv) for k,vv in v.items())\n",
    "        if hasattr(v,'dtype') and v.dtype==np.dtype('bool'):\n",
    "            v=v.astype(int)\n",
    "        data[key]= v\n",
    "\n",
    "    return data\n",
    "\n",
    "# test it works and saves as json\n",
    "# data=get_proxy_metadata(prxy)\n",
    "# s=json.dumps(data, cls=SetEncoder)\n",
    "# ss=json.loads(s)\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.669662",
     "start_time": "2016-12-12T17:26:47.654551"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.getLogger('export_makehuman').setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.685844",
     "start_time": "2016-12-12T17:26:47.671751"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Proxymeshes',\n",
       " 'Clothes',\n",
       " 'Hair',\n",
       " 'Eyes',\n",
       " 'Eyebrows',\n",
       " 'Eyelashes',\n",
       " 'Teeth',\n",
       " 'Tongue',\n",
       " 'Genitals']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T17:26:47.701200",
     "start_time": "2016-12-12T17:26:47.687410"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json\n"
     ]
    }
   ],
   "source": [
    "outdir = Path(tempfile.mkdtemp(suffix='Convert_proxy_to_threejs_json'))\n",
    "rig_file = 'data/rigs/default.mhskel'\n",
    "print(outdir)\n",
    "import material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-12T18:56:09.133651",
     "start_time": "2016-12-12T18:48:44.453948"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032 vertices, 4036 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/Harvey_Sweater2/Harvey_Sweater2.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3538 vertices, 3516 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/Harvey_MadScientistGlovesV1/Harvey_MadScientistGlovesV1.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1473 vertices, 1454 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/Harvey_PantsBootsV1/Harvey_PantsBootsV1.json\n",
      "nb_materials 1 nb_bones 326\n",
      "783 vertices, 708 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/Harvey_VestV1/Harvey_VestV1.json\n",
      "nb_materials 1 nb_bones 326\n",
      "834 vertices, 729 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/JeansSkirt/JeansSkirt.json\n",
      "nb_materials 1 nb_bones 326\n",
      "384 vertices, 360 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/armsleeve_fishnet_medium/armsleeve_fishnet_medium.json\n",
      "nb_materials 1 nb_bones 326\n",
      "384 vertices, 360 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/clothes/armsleeve_white_s/armsleeve_white_s.json\n",
      "nb_materials 1 nb_bones 326\n",
      "2196 vertices, 1096 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/afro01/afro01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1265 vertices, 1027 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/blondwithheadband/blondwithheadband.json\n",
      "nb_materials 1 nb_bones 326\n",
      "5203 vertices, 4237 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/bob01/bob01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1653 vertices, 1124 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/bob02/bob02.json\n",
      "nb_materials 1 nb_bones 326\n",
      "4493 vertices, 2759 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/Braid01/Braid01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "423 vertices, 374 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/Full_beard/Full_beard.json\n",
      "nb_materials 1 nb_bones 326\n",
      "6068 vertices, 5652 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/curly/curly.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3266 vertices, 3013 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/curly2/curly2.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1988 vertices, 1798 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/long2_alpha7/long2_alpha7.json\n",
      "nb_materials 1 nb_bones 326\n",
      "2258 vertices, 1944 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/long_alpha7/long_alpha7.json\n",
      "nb_materials 1 nb_bones 326\n",
      "249 vertices, 156 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/Harvey_Hawk1/Harvey_Hawk1.json\n",
      "nb_materials 1 nb_bones 326\n",
      "4885 vertices, 4288 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/junglebookhair/junglebookhair.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3239 vertices, 2054 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/long01/long01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1591 vertices, 1566 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/minoan/minoan.json\n",
      "nb_materials 1 nb_bones 326\n",
      "81 vertices, 60 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/Moustache/Moustache.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3718 vertices, 2676 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/ponytail01/ponytail01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "2984 vertices, 1839 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/short01/short01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1755 vertices, 1672 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/short02/short02.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1011 vertices, 961 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/short03/short03.json\n",
      "nb_materials 1 nb_bones 326\n",
      "865 vertices, 525 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/short04/short04.json\n",
      "nb_materials 1 nb_bones 326\n",
      "2747 vertices, 2684 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/southernbelleringlets/southernbelleringlets.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3573 vertices, 2686 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/hair/Wig_bun_blonde/Wig_bun_blonde.json\n",
      "nb_materials 1 nb_bones 326\n",
      "1064 vertices, 1020 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyes/HighPolyEyes/HighPolyEyes.json\n",
      "nb_materials 9 nb_bones 326\n",
      "96 vertices, 86 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyes/Low-Poly/Low-Poly.json\n",
      "nb_materials 9 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow001/eyebrow001.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow002/eyebrow002.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow003/eyebrow003.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow004/eyebrow004.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow005/eyebrow005.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow006/eyebrow006.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow007/eyebrow007.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow008/eyebrow008.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow009/eyebrow009.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow010/eyebrow010.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow011/eyebrow011.json\n",
      "nb_materials 1 nb_bones 326\n",
      "124 vertices, 96 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/eyebrow012/eyebrow012.json\n",
      "nb_materials 1 nb_bones 326\n",
      "11358 vertices, 8868 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyebrows/Eyebrows_01/Eyebrows_01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "250 vertices, 184 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyelashes/Eyelashes01/Eyelashes01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "250 vertices, 184 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyelashes/eyelashes02/eyelashes02.json\n",
      "nb_materials 1 nb_bones 326\n",
      "250 vertices, 184 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyelashes/eyelashes03/eyelashes03.json\n",
      "nb_materials 1 nb_bones 326\n",
      "250 vertices, 184 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyelashes/eyelashes04/eyelashes04.json\n",
      "nb_materials 1 nb_bones 326\n",
      "8316 vertices, 5544 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/eyelashes/Eyelashes_01/Eyelashes_01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3868 vertices, 3560 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/teeth/Teeth_Base/Teeth_Base.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3868 vertices, 3560 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/teeth/Teeth_Shape01/Teeth_Shape01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3868 vertices, 3560 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/teeth/Teeth_Shape02/Teeth_Shape02.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3868 vertices, 3560 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/teeth/Teeth_Shape03/Teeth_Shape03.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3868 vertices, 3560 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/teeth/Teeth_Shape04/Teeth_Shape04.json\n",
      "nb_materials 1 nb_bones 326\n",
      "3868 vertices, 3560 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/teeth/Teeth_Shape05/Teeth_Shape05.json\n",
      "nb_materials 1 nb_bones 326\n",
      "226 vertices, 224 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/tongue/tongue01/tongue01.json\n",
      "nb_materials 1 nb_bones 326\n",
      "859 vertices, 824 faces, 1 materials, 0 morph targets, 0 bones, 0 skinWeights\n",
      "/tmp/tmpmwDyyCConvert_proxy_to_threejs_json/genitals/penis01/penis01.json\n",
      "nb_materials 1 nb_bones 326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "proxyGroups = [\n",
    "#     # 'Proxymeshes',\n",
    "    'Clothes',\n",
    "    'Hair',\n",
    "    'Eyes',\n",
    "    'Eyebrows',\n",
    "    'Eyelashes',\n",
    "    'Teeth',\n",
    "    'Tongue',\n",
    "    'Genitals'\n",
    "]\n",
    "for group in proxyGroups:\n",
    "    for proxy_name in proxies[group]:\n",
    "        proxy_file = proxies[group][proxy_name]\n",
    "        \n",
    "        # ignore the debug proxies\n",
    "        if proxy_file.find('__') != -1: continue\n",
    "        \n",
    "        # load data\n",
    "        basehuman = getHuman()\n",
    "        humanargparser.addRig(basehuman, rig_file)\n",
    "        prxy = mhproxy.loadProxy(basehuman, proxy_file)\n",
    "        mesh, obj = prxy.loadMeshAndObject(basehuman)\n",
    "        \n",
    "        # export\n",
    "        infile = Path(prxy.obj_file)\n",
    "        outfile = outdir.joinpath(group.lower()).joinpath(prxy.name,prxy.name + '.json')\n",
    "        outfile.dirname().makedirs_p()\n",
    "        if outfile.isfile(): continue\n",
    "        \n",
    "        convert_ascii(\n",
    "            infile=infile,\n",
    "            morphfiles='',\n",
    "            colorfiles='',\n",
    "            outfile=outfile,\n",
    "            licence=json.dumps(LicenseInfo().asDict()),\n",
    "            mtllib=material_to_mtl(prxy.material, texdir=os.path.dirname(outfile))\n",
    "        )\n",
    "        print(outfile)\n",
    "        \n",
    "        # some extra data to add to the file\n",
    "        skeleton = basehuman.getSkeleton()\n",
    "        bones = parse_skeleton_bones(skeleton)\n",
    "        skeletonMetadata  = {\n",
    "            \"name\": skeleton.name,\n",
    "            \"version\": skeleton.version,\n",
    "            \"description\": skeleton.description,\n",
    "            \"plane_map_strategy\": skeleton.plane_map_strategy,\n",
    "            \"license\": skeleton.license.asDict(),\n",
    "        }\n",
    "        vertex_weights = prxy.getVertexWeights(skeleton.getVertexWeights())\n",
    "        influencesPerVertex = int(min(vertex_weights._nWeights, 4))\n",
    "        skinIndices, skinWeights = vertex_weights_to_skin_weights(vertex_weights, skeleton, influencesPerVertex=influencesPerVertex)\n",
    "        licence = json.dumps(mh_licence.asDict())\n",
    "        \n",
    "        # now add extra data to file\n",
    "        metadata = get_proxy_metadata(prxy)\n",
    "        metadata['skeletonMetadata']=skeletonMetadata\n",
    "        data = json.load(open(outfile))\n",
    "        data['metadata'].update(metadata)\n",
    "        \n",
    "        data['skinIndices']=skinIndices\n",
    "        data['skinWeights']=skinWeights\n",
    "        data['offsets']=prxy.offsets\n",
    "        data['ref_vIdxs']=prxy.ref_vIdxs\n",
    "        data['weights']=prxy.weights\n",
    "        data['bones']=bones\n",
    "        data['influencesPerVertex']=influencesPerVertex\n",
    "        \n",
    "        assert len(data['ref_vIdxs'])==len(data['weights'])\n",
    "        assert len(data['ref_vIdxs'])>0\n",
    "        assert len(data['offsets'])>0\n",
    "        assert len(data['skinIndices'])==len(data['skinIndices'])\n",
    "        assert len(data['skinIndices'])>0\n",
    "        assert len(data['skinIndices'])%data['influencesPerVertex']==0\n",
    "        \n",
    "        \n",
    "        # load alternative materials\n",
    "        materials=[]\n",
    "        if prxy.material_file:\n",
    "            for material_file in Path(prxy.material_file).dirname().glob('*.mhmat'):\n",
    "                material_name = str(Path(material_file).basename().splitext()[0])\n",
    "                mat = material.Material(material_name)\n",
    "                mat.fromFile(material_file)\n",
    "                mtl = parse_mtl(material_to_mtl(mat, texdir=os.path.dirname(outfile)))\n",
    "                mtl = mtl[mtl.keys()[0]]\n",
    "                mtl['name']=material_name\n",
    "                materials.append(mtl)\n",
    "        data[\"materials\"] = materials\n",
    "        \n",
    "        json.dump(data, open(outfile, 'w'), cls=NP_MH_Encoder, separators=(',', ':'))\n",
    "        \n",
    "        \n",
    "        # copy thumbnail\n",
    "        thumbnail = Path(infile.replace('.obj','.thumb'))\n",
    "        if thumbnail.isfile():\n",
    "            copyAndCompress(thumbnail,outfile.replace('.json','.thumb.png'))\n",
    "        for thumbnail in [p.replace('.mhmat','.thumb') for p in Path(prxy.material_file).dirname().glob('*.mhmat')]:\n",
    "            if Path(thumbnail).isfile():\n",
    "                outnail = outfile.dirname().joinpath(Path(thumbnail).basename()).replace('.thumb','.thumb.png')\n",
    "                copyAndCompress(thumbnail,outnail)\n",
    "        \n",
    "        print('nb_materials {nb_materials:} nb_bones {nb_bones:}'\n",
    "          .format(\n",
    "            nb_materials=len(materials),\n",
    "            nb_bones=len(bones),\n",
    "            nb_ref_vIdxs=len(data['ref_vIdxs'])\n",
    "         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
